{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e321237",
   "metadata": {},
   "source": [
    "### 1. What is Simple Linear Regression?\n",
    "Simple Linear Regression is a statistical technique used to model the relationship between one independent variable (X) and a dependent variable (Y).\n",
    "It fits a straight line that best represents the data using the least squares method.\n",
    "The primary goal is to predict Y based on X and understand how changes in X influence Y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28293b2",
   "metadata": {},
   "source": [
    "### 2. What are the key assumptions of Simple Linear Regression?\n",
    "The model assumes a linear relationship between X and Y, meaning changes in X correspond to proportional changes in Y.\n",
    "Residuals must be normally distributed with constant variance (homoscedasticity) and should be independent of each other.\n",
    "Additionally, the predictor variable should be measured without significant error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ed271",
   "metadata": {},
   "source": [
    "### 3. What does the coefficient m represent in Y = mX + c?\n",
    "The coefficient m is the slope of the regression line and represents the rate of change in Y for each unit change in X.\n",
    "A positive slope indicates that Y increases as X increases, while a negative slope indicates the opposite.\n",
    "The magnitude of m shows how strong the relationship is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e3710",
   "metadata": {},
   "source": [
    "### 4. What does the intercept c represent in Y = mX + c?\n",
    "The intercept c represents the predicted value of Y when X is zero.\n",
    "It determines the vertical placement of the regression line on the graph.\n",
    "Its interpretation depends on whether X=0 is meaningful within the context of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db683ef",
   "metadata": {},
   "source": [
    "### 5. How do we calculate the slope m in Simple Linear Regression?\n",
    "The slope m is calculated using the formula m = Cov(X, Y) / Var(X), derived from minimizing squared residuals.\n",
    "This ensures the best-fitting line by optimizing the vertical error between actual and predicted values.\n",
    "The slope indicates how sensitive Y is to changes in X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be3237",
   "metadata": {},
   "source": [
    "### 6. What is the purpose of the least squares method?\n",
    "The least squares method minimizes the sum of squared differences between observed and predicted values.\n",
    "It ensures the regression line fits the data as accurately as possible.\n",
    "This method also gives slope and intercept estimates with desirable statistical properties under model assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fb82ef",
   "metadata": {},
   "source": [
    "### 7. How is R² interpreted in Simple Linear Regression?\n",
    "R² indicates the proportion of variation in the dependent variable explained by the independent variable.\n",
    "It ranges from 0 to 1, where higher values show a better fit of the model.\n",
    "However, it does not indicate causal relationship or prediction quality on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8f344",
   "metadata": {},
   "source": [
    "### 8. What is Multiple Linear Regression?\n",
    "Multiple Linear Regression models the relationship between multiple independent variables and one dependent variable.\n",
    "It extends simple regression by estimating separate coefficients for each predictor.\n",
    "This helps understand combined and individual effects of predictors on the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694c51a",
   "metadata": {},
   "source": [
    "### 9. Main difference between Simple and Multiple Linear Regression?\n",
    "Simple Linear Regression uses one predictor, while Multiple Linear Regression uses two or more.\n",
    "Multiple regression captures more complex relationships and interactions among predictors.\n",
    "However, it requires stricter assumptions and careful interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39316726",
   "metadata": {},
   "source": [
    "### 10. Key assumptions of Multiple Linear Regression?\n",
    "The model assumes linearity between predictors and outcome, normally distributed and homoscedastic residuals, and independence of observations.\n",
    "It also requires no perfect multicollinearity among predictors.\n",
    "Model correctness depends on including all relevant variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a94bee",
   "metadata": {},
   "source": [
    "### 11. What is heteroscedasticity?\n",
    "Heteroscedasticity occurs when residual variance changes across levels of predictors rather than remaining constant.\n",
    "It leads to unreliable standard errors, affecting hypothesis tests and confidence intervals.\n",
    "Although coefficient estimates remain unbiased, inference becomes inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0ae870",
   "metadata": {},
   "source": [
    "### 12. How to improve a model with high multicollinearity?\n",
    "You can remove or combine highly correlated predictors, or transform them using PCA.\n",
    "Regularization techniques such as Ridge or Lasso regression also help stabilize coefficient estimates.\n",
    "Increasing sample size or centering variables may reduce multicollinearity effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae27d108",
   "metadata": {},
   "source": [
    "### 13. Techniques for transforming categorical variables?\n",
    "One-hot encoding creates binary variables for each category, commonly used in regression.\n",
    "Label encoding assigns numeric values to ordinal features.\n",
    "For high-cardinality variables, target encoding or frequency encoding may be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01771391",
   "metadata": {},
   "source": [
    "### 14. Role of interaction terms?\n",
    "Interaction terms capture situations where the effect of one predictor on Y depends on another predictor.\n",
    "They help model non-additive relationships, adding flexibility to the regression equation.\n",
    "They are essential when theory or data suggests variable interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c66d4",
   "metadata": {},
   "source": [
    "### 15. Interpretation of intercept in Simple vs Multiple Regression?\n",
    "In Simple Regression, the intercept is the predicted value of Y at X=0.\n",
    "In Multiple Regression, the intercept represents Y when all predictors equal zero.\n",
    "Depending on context, this may or may not be meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f709017c",
   "metadata": {},
   "source": [
    "### 16. Significance of the slope in regression?\n",
    "The slope indicates how much the dependent variable changes for a unit increase in the predictor.\n",
    "It tells us the strength and direction of the relationship.\n",
    "Larger absolute values imply stronger influence on predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff523ce",
   "metadata": {},
   "source": [
    "### 17. How does the intercept provide context?\n",
    "The intercept establishes the baseline value of Y when predictors are zero.\n",
    "It helps anchor the regression line and interpret predictions.\n",
    "Its practical relevance depends on whether zero values make sense for the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d8937",
   "metadata": {},
   "source": [
    "### 18. Limitations of R²?\n",
    "R² does not indicate whether the model is appropriate or overfitted.\n",
    "It also cannot assess predictive performance on unseen data.\n",
    "High R² does not guarantee meaningful or reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6299d281",
   "metadata": {},
   "source": [
    "### 19. Interpretation of large standard error?\n",
    "A large standard error means the coefficient estimate is unstable and uncertain.\n",
    "It suggests the model may suffer from multicollinearity or insufficient data.\n",
    "High SE reduces statistical significance of the coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded5afdd",
   "metadata": {},
   "source": [
    "### 20. Identifying heteroscedasticity in residual plots?\n",
    "Residual plots show heteroscedasticity when residuals form a fan or cone shape.\n",
    "Addressing it is important because it biases standard errors and test statistics.\n",
    "Solutions include transformations or using robust standard errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676adf65",
   "metadata": {},
   "source": [
    "### 21. Meaning of high R² but low adjusted R²?\n",
    "This indicates the model contains unnecessary predictors that don't improve explanatory power.\n",
    "The model may be overfitted, capturing noise rather than signal.\n",
    "Adjusted R² penalizes extra predictors, revealing this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0588dd88",
   "metadata": {},
   "source": [
    "### 22. Why scale variables in regression?\n",
    "Scaling ensures all predictors contribute equally during optimization.\n",
    "It improves stability in algorithms sensitive to variable magnitude.\n",
    "It is essential for regularization methods like Ridge and Lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00898e8",
   "metadata": {},
   "source": [
    "### 23. What is polynomial regression?\n",
    "Polynomial regression models nonlinear relationships by adding polynomial terms (X², X³...).\n",
    "It maintains linearity in coefficients while allowing curved fits.\n",
    "Useful for capturing smooth nonlinear trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd1bbf",
   "metadata": {},
   "source": [
    "### 24. Difference between polynomial and linear regression?\n",
    "Linear regression fits a straight line, whereas polynomial fits curved patterns.\n",
    "Polynomial regression introduces higher-degree terms to capture complexity.\n",
    "It remains a linear model in terms of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df22295",
   "metadata": {},
   "source": [
    "### 25. When is polynomial regression used?\n",
    "Used when data shows a curved or nonlinear pattern that a straight line cannot capture.\n",
    "Appropriate for trend analysis, growth curves, and smooth nonlinear relationships.\n",
    "Care must be taken to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d46b7",
   "metadata": {},
   "source": [
    "### 26. General equation for polynomial regression?\n",
    "The equation is Y = b0 + b1X + b2X² + ... + bnXⁿ.\n",
    "Each added term increases model flexibility.\n",
    "Higher-degree polynomials allow more curvature in the fitted line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb41fa",
   "metadata": {},
   "source": [
    "### 27. Can polynomial regression apply to multiple variables?\n",
    "Yes, polynomial terms can be added for each predictor and their interactions.\n",
    "This results in complex models with many terms.\n",
    "Regularization is often required to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da508581",
   "metadata": {},
   "source": [
    "### 28. Limitations of polynomial regression?\n",
    "High-degree polynomials often overfit the data.\n",
    "They are sensitive to outliers and extrapolate poorly outside the data range.\n",
    "Interpretability decreases as model complexity grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c9925",
   "metadata": {},
   "source": [
    "### 29. Selecting polynomial degree?\n",
    "Use cross-validation to compare performance across degrees.\n",
    "AIC/BIC scores and validation curves help choose optimal complexity.\n",
    "Visualization of fit and residuals also guides selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43da59d",
   "metadata": {},
   "source": [
    "### 30. Why is visualization important?\n",
    "Plots help assess how well the polynomial curve fits the data.\n",
    "They reveal underfitting, overfitting, and anomalies.\n",
    "Visualization improves interpretation and communication of results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4fccb7",
   "metadata": {},
   "source": [
    "### 31. How is polynomial regression implemented in Python?\n",
    "Polynomial regression in Python uses PolynomialFeatures to expand features.\n",
    "Then a LinearRegression model is fit to the transformed data.\n",
    "A simple example is included below in a code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c442f490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.00000000e+00 2.10942375e-15 1.00000000e+00]\n",
      "Intercept: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([1,2,3,4]).reshape(-1,1)\n",
    "y = np.array([2,5,10,17])\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"Coefficients:\", model.named_steps['linearregression'].coef_)\n",
    "print(\"Intercept:\", model.named_steps['linearregression'].intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9127332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
